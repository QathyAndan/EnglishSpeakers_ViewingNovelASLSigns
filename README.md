# English Speakers_Viewing Novel ASL Signs
This is the pipeline that I built for my dissertation work over a period from 2019-2021. The pipeline takes raw eye movement data in the form of gaze coordinate pairs (X,Y) sampled over time, and conducts a number of statistical analyses on the data. The pipeline starts with a raw csv, then the CSV is manipulated in R, the dataset is parsed into MATLAB variables (because of a legacy series of code files that I kept consistent with a prior method, to ensure comparability to prior research) and those variables are loaded into and analyzed in R (tidyR, dplyr). 

Context:
The analysis compares how participants looked novel words or signs, to see if English speakers' looking behavior was sensitive to differences in phonological structure between novel words, and then to see if evidence of that same sensitivity to phonological structure could be extended not only to spoken words but also to novel American Sign Language signs. English speakers were shown pairs of words, one pair at a time, and were asked to choose which word would make a better word (in English, or a better sign in ASL). All participants were naieve to sign language and knew no ASL signs. While they looked at word pairs and made their choice with a key press, their eye movements as they watched the stimuli were recorded. Within a pair, words (or signs) were matched phonological apart from one phonological characteristic. The experimental data for this project were conducted online remotely during the COVID pandemic. Participants were recruited via Prolific, and all stimuil and eye tracking were done using remote native-web-camera-based eye tracking via Labvanced. Measures collected and analyzed included fixation durations, total looking time, # regressions (number of times a participant looked at a target again after the first time), and the pattern of looking behavior over time modeled as a logistic polynomial regression using hierchical modeling. 


